## Configuration options to run train.py
epochs: 200
patience: 40
#dropout: 0.1

model_name: "Word_level_predictor"
lr: 0.01
teacher_forcing_ratio: 0
scheduler: "MultiStepLR"
gamma: 0.1
epoch_freeze: 50
freeze: True
milestones: [3, 5, 8, 10, 15, 30, 50, 80, 100, 120, 140]

network: 
  checkpoint: ~

datasets: 
  train: 
    batch_size: 128
    shuffle: True
    num_workers: 2

  valid:
    batch_size: 130
    shuffle: True
    num_workers: 2

  test:
    batch_size: 130
    shuffle: False
    num_workers: 2


wandb: False
  # resume: None
  # id: qu3tryju