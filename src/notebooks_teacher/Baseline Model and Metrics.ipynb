{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f28f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fhome/gia07/anaconda3/envs/medImg/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from transformers import ResNetModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b723e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "base_path = '/fhome/gia07/Image_captioning/src/Dataset/'\n",
    "\n",
    "img_path = f'{base_path}Images/'\n",
    "cap_path = f'{base_path}captions.txt'\n",
    "path_partitions = f'{base_path}flickr8k_partitions.npy'\n",
    "\n",
    "data = pd.read_csv(cap_path)\n",
    "partitions = np.load(path_partitions, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b09b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['<SOS>', '<EOS>', '<PAD>', ' ', '!', '\"', '#', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "NUM_CHAR = len(chars)\n",
    "idx2char = {k: v for k, v in enumerate(chars)}\n",
    "char2idx = {v: k for k, v in enumerate(chars)}\n",
    "\n",
    "TEXT_MAX_LEN = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff6590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data, partition):\n",
    "        self.data = data\n",
    "        self.partition = partition\n",
    "        self.num_captions = 5\n",
    "        self.max_len = TEXT_MAX_LEN\n",
    "        self.img_proc = torch.nn.Sequential(\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((224, 224), antialias=True),\n",
    "            v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.partition)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.num_captions*self.partition[idx]\n",
    "        item = self.data.iloc[real_idx: real_idx+self.num_captions]\n",
    "        ## image processing\n",
    "        img_name = item.image.reset_index(drop=True)[0]\n",
    "        img = Image.open(f'{img_path}{img_name}').convert('RGB')\n",
    "        img = self.img_proc(img)\n",
    "    \n",
    "        ## caption processing\n",
    "        caption = item.caption.reset_index(drop=True)[random.choice(list(range(self.num_captions)))]\n",
    "        cap_list = list(caption)\n",
    "        final_list = [chars[0]]\n",
    "        final_list.extend(cap_list)\n",
    "        final_list.extend([chars[1]])\n",
    "        gap = self.max_len - len(final_list)\n",
    "        final_list.extend([chars[2]]*gap)\n",
    "        cap_idx = [char2idx[i] for i in final_list]\n",
    "        return img, cap_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b652abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)\n",
    "        self.gru = nn.GRU(512, 512, num_layers=1)\n",
    "        self.proj = nn.Linear(512, NUM_CHAR)\n",
    "        self.embed = nn.Embedding(NUM_CHAR, 512)\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.shape[0]\n",
    "        feat = self.resnet(img)\n",
    "        feat = feat.pooler_output.squeeze(-1).squeeze(-1).unsqueeze(0) # 1, batch, 512\n",
    "        start = torch.tensor(char2idx['<SOS>']).to(DEVICE)\n",
    "        start_embed = self.embed(start) # 512\n",
    "        start_embeds = start_embed.repeat(batch_size, 1).unsqueeze(0) # 1, batch, 512\n",
    "        inp = start_embeds\n",
    "        hidden = feat\n",
    "        for t in range(TEXT_MAX_LEN-1): # rm <SOS>\n",
    "            out, hidden = self.gru(inp, hidden)\n",
    "            inp = torch.cat((inp, out[-1:]), dim=0) # N, batch, 512\n",
    "    \n",
    "        res = inp.permute(1, 0, 2) # batch, seq, 512\n",
    "        res = self.proj(res) # batch, seq, 80\n",
    "        res = res.permute(0, 2, 1) # batch, 80, seq\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82acf476",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/fhome/gia07/Image_captioning/Baseline Model and Metrics.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((img1\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), img2\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m caption \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((caption1\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), caption2\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m img, caption \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mto(DEVICE), caption\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m Model()\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m pred \u001b[39m=\u001b[39m model(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/medImg/lib/python3.11/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "'''A simple example to calculate loss of a single batch (size 2)'''\n",
    "dataset = Data(data, partitions['train'])\n",
    "img1, caption1 = next(iter(dataset))\n",
    "img2, caption2 = next(iter(dataset))\n",
    "caption1 = torch.tensor(caption1)\n",
    "caption2 = torch.tensor(caption2)\n",
    "img = torch.cat((img1.unsqueeze(0), img2.unsqueeze(0)))\n",
    "caption = torch.cat((caption1.unsqueeze(0), caption2.unsqueeze(0)))\n",
    "img, caption = img.to(DEVICE), caption.to(DEVICE)\n",
    "model = Model().to(DEVICE)\n",
    "pred = model(img)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "loss = crit(pred, caption)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3606f2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/fhome/gia07/Image_captioning/Baseline Model and Metrics.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'''metrics'''\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m bleu \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mbleu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m meteor \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmeteor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566973696f6e227d/fhome/gia07/Image_captioning/Baseline%20Model%20and%20Metrics.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m rouge \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "'''metrics'''\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "reference = [['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .']]\n",
    "prediction = ['A girl goes into a wooden building .']\n",
    "\n",
    "res_b = bleu.compute(predictions=prediction, references=reference)\n",
    "res_r = rouge.compute(predictions=prediction, references=reference)\n",
    "res_m = meteor.compute(predictions=prediction, references=reference)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7595f1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.4723665527410147,\n",
       "  'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       "  'brevity_penalty': 0.4723665527410147,\n",
       "  'length_ratio': 0.5714285714285714,\n",
       "  'translation_length': 4,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.7272727272727273,\n",
       "  'rouge2': 0.6666666666666666,\n",
       "  'rougeL': 0.7272727272727273,\n",
       "  'rougeLsum': 0.7272727272727273},\n",
       " {'meteor': 0.5923507462686567})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is running']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe24da77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 1.0, 1.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.5, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.44612794612794615})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca1afd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 0.5, 0.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.25, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.3872053872053872},\n",
       " {'meteor': 0.45454545454545453})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "res_m_sin = meteor.compute(predictions=pred1, references=ref, gamma=0) # no penalty by setting gamma to 0\n",
    "\n",
    "res_b, res_r, res_m, res_m_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bac05",
   "metadata": {},
   "source": [
    "Final metric we use for challenge 3: BLEU1, BLEU2, ROUGE-L, METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5433823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BLEU-1:26.4%, BLEU2:18.6%, ROUGE-L:60.0%, METEOR:38.7%'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "bleu1 = bleu.compute(predictions=pred1, references=ref, max_order=1)\n",
    "bleu2 = bleu.compute(predictions=pred1, references=ref, max_order=2)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "f\"BLEU-1:{bleu1['bleu']*100:.1f}%, BLEU2:{bleu2['bleu']*100:.1f}%, ROUGE-L:{res_r['rougeL']*100:.1f}%, METEOR:{res_m['meteor']*100:.1f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf5d9d",
   "metadata": {},
   "source": [
    "Now it is your turn! Try to finish the code below to run the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9835fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(EPOCHS):\n",
    "    data_train = Data(data, partitions['train'])\n",
    "    data_valid = Data(data, partitions['valid'])\n",
    "    data_test = Data(data, partitions['test'])\n",
    "    dataloader_train = '''write a proper dataloader, same for valid and test'''\n",
    "    model = Model().to(DEVICE)\n",
    "    model.train()\n",
    "    optimizer = '''choose a proper optimizer'''\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    metric = Metric()\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss, res = train_one_epoch(model, optimizer, crit, metric, dataloader_train)\n",
    "        print(f'train loss: {loss:.2f}, metric: {res:.2f}, epoch: {epoch}')\n",
    "        loss_v, res_v = eval_epoch(model, crit, metric, dataloader_valid)\n",
    "        print(f'valid loss: {loss:.2f}, metric: {res:.2f}')\n",
    "    loss_t, res_t = eval_epoch(model, crit, metric, testloader_test)\n",
    "    print(f'test loss: {loss:.2f}, metric: {res:.2f}')\n",
    "    \n",
    "def train_one_epoch(model, optimizer, crit, metric, dataloader):\n",
    "    '''finish the code'''\n",
    "    return loss, res\n",
    "\n",
    "def eval_epoch(model, crit, metric, dataloader):\n",
    "    '''finish the code'''\n",
    "    return loss, res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
