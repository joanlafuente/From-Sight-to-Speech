epochs: 150
# lr: 0.001

network: 
  checkpoint: ~

  freeze_encoder: True           # True, False
  epoch2unfreeze: 20            # epoch to unfreeze encoder
  save_ckpt_every: 3

  params: 
    type: "VIT_transformer_updated_pretrained"  # Bl_gru, Bl_lstm <- de moment aixo (gru_attention, lstm_attention...)
    teacher_forcing_ratio: 1      # number between 0 and 1
    dropout: 0.1                    # number between 0 and 1
    text_max_len: 40         # max number of chr/words in a sentence
    transformer_layers: 2            # number of transformer layers
    transformer_heads: 3             # number of transformer heads    
    return_attn: False 
    pretrained_embedding: True           

scheduler: 
  type: MultiStepLR                         # MultiStepLR or ~     
  milestones: [50, 75, 120, 160]        # milestones for MultiStepLR
  gamma: 0.1                      # gamma for MultiStepLR

loss: 
  type: "CrossEntropyLoss"         # CrossEntropyLoss, NLLLoss
  weights: "penalize_pad"

optimizer: 
  type: "Adam"                     # Adam, SGD
  weight_decay: 0.0001
  lr: 0.001

early_stopping:
  metric: "bleu2"                 # loss, bleu, rouge
  patience: 250                    # number of epochs to wait before early stopping

datasets:
  type: Data_word_1cap                 # Data_char, Data_word
  base_path: '/fhome/gia07/Image_captioning/src/Dataset/'
  augment_imgs: True

  train:
    batch_size: 25
    shuffle: True
    num_workers: 2

  valid:
    batch_size: 25
    shuffle: True
    num_workers: 2

  test:
    batch_size: 25
    shuffle: False
    num_workers: 2