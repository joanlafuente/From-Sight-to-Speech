epochs: 120

network: 
  checkpoint: ~                   # path to checkpoint or ~
  
  freeze_encoder: True           # True, False
  epoch2unfreeze: 60            # epoch to unfreeze encoder
  save_ckpt_every: 3

  params: 
    type: "lstm_convnext_tiny_bahdanau_input_multiple_layers_hope_linear"  # Bl_gru, Bl_lstm <- de moment aixo (gru_attention, lstm_attention...)
    teacher_forcing_ratio: 0      # number between 0 and 1
    dropout: 0.15                    # number between 0 and 1
    text_max_len: 40         # max number of chr/words in a sentence
    rnn_layers: 4                 # number of rnn layers

scheduler: 
  type: "MultiStepLR"                         # MultiStepLR or ~     
  milestones: [50, 75, 100, 120, 160]        # milestones for MultiStepLR
  gamma: 0.1                      # gamma for MultiStepLR

optimizer: 
  type: "Adam"                     # Adam, SGD
  weight_decay: 0.0001
  lr: 0.001                      # learning rate

loss: 
  type: "CrossEntropyLoss"         # CrossEntropyLoss, NLLLoss
  weights: False

early_stopping:
  metric: "bleu2"                 # loss, bleu, rouge
  patience: 250                    # number of epochs to wait before early stopping

datasets:
  type: Data_word_1cap                 # Data_char, Data_word
  base_path: '/fhome/gia07/Image_captioning/src/Dataset/'
  augment_imgs: True

  train:
    batch_size: 400   
    shuffle: True
    num_workers: 2

  valid:
    batch_size: 400
    shuffle: True
    num_workers: 2

  test:
    batch_size: 400
    shuffle: False
    num_workers: 2